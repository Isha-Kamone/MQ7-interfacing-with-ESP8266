## What Machine Learning Actually Is
- Fundamentally, machine learning is a technique for using computers to predict things based on past observations. We collect data and then create a computer program that analyzes that data and uses it to predict future states.
- Creating a machine learning program is different from the usual process of writing code. In a traditional piece of software, a programmer 
designs an algorithm that takes an input, applies various rules, and returns an output. 
- For Ex. To predict breakdowns in a factory machine, the programmer would need to understand which measurements in the data indicate a problem 
and write code that deliberately checks for them. There might be various different combinations of production rate, temperature, and vibration level that might indicate a
problem but are not immediately obvious from looking at the data.
- To create a machine learning program, we feed data into a algorithm and then the algorithm discover the rules. This means that we can create 
programs that make predictions based on complex data without having to understand all of the complexity ourselves. The machine learning algorithm
 builds a model of the system based on the data we provide, through a process we call training. We run data through this model to make predictions, in a process called inference.
- There are many different approaches to machine learning. One of the most popular is deep learning, which is based on a simplified idea of how the human brain might
work. In deep learning, a network of neurons (represented by arrays of numbers) is trained to model the relationships between various inputs and outputs.
Some architectures in the network excel at extracting meaning from image data, while other architectures work  for predicting the next value in a sequence.


## The Deep Learning Workflow :
###### Deciding  on a Goal : 
- When designing  algorithm, we need to decide what we want to predict so we can decide what data to collect and which
model architecture to use.
- In factory machine example, we want to predict whether factory machine is about to break down. This as a classification problem in which machine
learning task takes a set of input data and returns the probability that this data fits each of a set of known classes. In this example, we might have two classes: 
1. “normal,” meaning that our machine is operating without issue
2. “abnormal,” meaning that our machine is showing signs that it might soon break down. 
This means that our goal is to create a model that classifies our input data as either “normal” or “abnormal


###### Collecting a Dataset :
- How much data is required to train a model is depends on factors like the complexity of the relationships between variables, the amount of noise.
But the more data, the better! We should aim to collect data that represents the full range of conditions and events that can occur in the system. 
- If our machine can fail in several different ways, we should be sure to capture data around each type of failure. If a variable changes naturally over time, 
it’s important to collect data that represents the full range. For example, if the machine’s temperature rises on warm days, you should be sure to include
data from both winter and summer. This  will help  model to represent every possible scenario.
######Labeling data :
- In addition to collecting data, we need to determine which data represents “normal” and “abnormal” operation. The process of associating data
with classes is called labeling, and the “normal” and “abnormal” classes are our labels

###### Design a Model Architecture :
- We also need to think about the constraints of the device we will be running the model on, because microcontrollers have limited memory and slow processors, and 
larger models require more memory and take more time to run—the size of a model depends on the number of neurons it contains, and the way those neurons
are connected. 
- We might start by training a simple model with a few layers of neurons and then refining the architecture in an iterative process until we get a useful result.

Deep learning models accept input and generate output in the form of tensors. A tensor is essentially a list of numbers or other tensors, similar to array
The structure of a tensor is known as its shape, and they come in multiple dimensions.

**Vector** : 
A vector is a list of numbers, similar to an array. It’s the name we give a tensor
with a single dimension . 

**Matrix** : 
A matrix is a 2D tensor, similar to a 2D array. The following matrix is of shape

**Higher-dimensional tensors** : 
Any shape with more than two dimensions is just referred to as a tensor. Here’s a

**Scalar** :
A single number, known as a scalar, is technically a zero-dimensional tensor. For


###### Generating features from data :  
- In machine learning, the term feature refers to a particular type of information on which a model is trained. 
Different types of models are trained on different types of features.  For example, a model might accept a single scalar value as its input feature.
- But inputs can be much more complex than this. a model designed to process images might accept a multidimensional tensor of image data as its input, 
and a model designed to predict based on multiple features might accept a vector containing multiple scalar values, one for each feature.


###### Normalization
- The data feed into a neural network will be in the form of tensors filled with floating-point values.  For the training algorithm to work effec‐
tively, these floating-point values need to be similar in size to one another. It’s ideal if all values are expressed as numbers in the range of 0 to 1.
when images are fed into a neural network. 
- Computers often store images as matrices of 8-bit integers, whose values range from 0 to 255. To normalize
these values so that they are all between 0 and 1, each 8-bit value is multiplied by 1/255.

###### Training the Model
- The model’s weights start out with random values, and biases start with a value of 0. During training, batches of data are fed into the model, and the model’s
output is compared with the desired output (which in factory machine example is the correct label, “normal” or “abnormal”). And then backpropagation adjusts the weights
and biases incrementally so that  the output of the model gets closer to matching the desired value. Training, which is measured in epochs (meaning iterations), continues until we decide to stop.

###### Splitting of data
- A dataset is split into three parts—
training -the training dataset is used to train the model
validation - validation dataset is fed through the model, and the loss is calculated.
test 
A typical split is 60% training data, 20% validation, and 20% test.

###### Hyperparameters
- To improve the model’s performance, we can change our model architecture, and we can adjust various values used to set up the model and moderate the
training process like epochs, learning rate. These values are known as hyperparameters, 
At the point that model begins to make accurate predictions, it is said to have converged. To determine whether a model has converged, we can analyze loss and accuracy 
graphs of its performance during training. 


###### Underfitting and overfitting
- The two most common reasons a model fails to converge are underfitting and overfitting
- When a model is underfit, it has not yet been able to learn a strong enough representation of these patterns to be able to make good predictions. This can happen for a variety of reasons, most commonly that the architecture is too small to capture the complexity of the system it is supposed to model or that it has not been trained on enough data.
- When a model is overfit, it has learned its training data too well. The model is able to exactly predict the minutiae of its training data, but it is not able to generalize its learning to data it has not previously seen. Often this happens because the model has
managed to entirely memorize the training data.
- There are many ways to fight overfitting. One possibility is to reduce the size of the model so it does not have enough capacity to learn an exact representation of its
training set
- A set of techniques known as regularization can be applied during training to reduce the degree of overfitting.

###### Data Augmentation
- Data augmentation is a way to artificially exapand the size of a training dataset versions. This is done by creating multiple additional versions of 
creating multiple additional versions of every training input, each transformed in a way that preserves its meaning but varies its exact composition. In one of our examples, we train a model to recognize speech from
audio samples. We augment our original training data by adding artificial background noise and shifting the samples around in time.

